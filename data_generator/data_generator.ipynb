{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's generate some data to play with in Scala\n",
    "\n",
    "Three datasets:\n",
    "- Products\n",
    "- Sensors\n",
    "- Locations\n",
    "- Readings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate products dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Chat-GPT generated list of products\n",
    "with open(\"products.txt\", \"r\", encoding=\"UTF-8\") as products_file:\n",
    "    all_products = products_file.read().strip().split(\"\\n\")\n",
    "    all_products_dataset = [[i+1, all_products[i]] for i in range(len(all_products))]\n",
    "    all_products_df = pd.DataFrame(all_products_dataset, columns=[\"product_id\", \"product_description\"])\n",
    "    all_products_df.to_csv(\"output/products.csv\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Sensors dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat-GPT generated list of sensor_types with plant names\n",
    "with open(\"sensor_types.txt\", \"r\", encoding=\"UTF-8\") as sensor_types_file:\n",
    "    all_sensor_types = sensor_types_file.read().strip().split(\"\\n\")\n",
    "    all_sensor_types_dataset = [[i+1, all_sensor_types[i]] for i in range(len(all_sensor_types))]\n",
    "    all_sensor_types_df = pd.DataFrame(all_sensor_types_dataset, columns=[\"sensor_type\", \"sensor_description\"])\n",
    "    all_sensor_types_df.to_csv(\"output/sensor_types.csv\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate locations dataset\n",
    "\n",
    "Two-dimensional 500x500 space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "all_locations = []\n",
    "locations_dataset = []\n",
    "\n",
    "for i in range(10):\n",
    "    location = (random.randint(1, 500), random.randint(1, 500))\n",
    "    while location in all_locations:\n",
    "        location = (random.randint(1, 500), random.randint(1, 500))\n",
    "\n",
    "    all_locations.append(location)\n",
    "    locations_dataset.append([i+1, location[0], location[1]])\n",
    "\n",
    "locations_df = pd.DataFrame(locations_dataset, columns=[\"location_id\", \"x\", \"y\"])\n",
    "locations_df.to_csv(\"output/locations.csv\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate distance helper\n",
    "Using euclidean distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "distances_helper = {}\n",
    "\n",
    "for i in locations_dataset:\n",
    "    for j in locations_dataset:\n",
    "        if i[0] != j[0]:\n",
    "            distances_helper[(i[0], j[0])] = math.dist((i[1], i[2]), (j[1], j[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create events in which a product is transported from one location to another\n",
    "\n",
    "following all locations in-between, using the shortest path.\n",
    "\n",
    "There is a 1% chance of the product to be lost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EVENTS = 100000\n",
    "\n",
    "events_buckets = []\n",
    "\n",
    "# Event ID, Event Type, Product ID, Sensor ID, Source Location ID, Destination Location ID, Current Location ID\n",
    "process_id = 1\n",
    "for _ in range(MAX_EVENTS):\n",
    "    product_id = random.randint(1, len(all_products_dataset))\n",
    "    sensor_type_id = random.randint(1, len(all_sensor_types_dataset))\n",
    "    current_location_id = random.randint(1, len(locations_dataset))\n",
    "    destination_location_id = random.randint(1, len(locations_dataset))\n",
    "    \n",
    "    while (destination_location_id == current_location_id):\n",
    "        destination_location_id = random.randint(1, len(locations_dataset))\n",
    "\n",
    "    event_bucket = []\n",
    "\n",
    "    event_bucket.append([process_id, \"START\", product_id, sensor_type_id, current_location_id, destination_location_id, current_location_id])\n",
    "\n",
    "    mark_as_lost = False\n",
    "\n",
    "    while current_location_id != destination_location_id and not mark_as_lost:\n",
    "        destination_distance = distances_helper[(current_location_id, destination_location_id)]\n",
    "        current_distances = {k[1]: v for k, v in distances_helper.items() if k[0] == current_location_id}\n",
    "        sorted_current_distances = dict(sorted(current_distances.items(), key=lambda item: item[1]))\n",
    "\n",
    "        for destination in sorted_current_distances:\n",
    "\n",
    "            if destination != destination_location_id:\n",
    "\n",
    "                if distances_helper[(destination, destination_location_id)] <= destination_distance:\n",
    "                    current_location_id = destination\n",
    "\n",
    "                    mark_as_lost = random.random() < 0.01\n",
    "                    \n",
    "                    event_bucket.append([process_id, \"MOVE\", product_id, sensor_type_id, current_location_id, destination_location_id, current_location_id])\n",
    "                    break\n",
    "\n",
    "            else:\n",
    "                current_location_id = destination\n",
    "                event_bucket.append([process_id, \"END\", product_id, sensor_type_id, current_location_id, destination_location_id, current_location_id])\n",
    "                break\n",
    "\n",
    "\n",
    "    events_buckets.append(event_bucket)\n",
    "\n",
    "    process_id += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scramble events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_events = []\n",
    "\n",
    "from copy import deepcopy\n",
    "events_buckets_copy = deepcopy(events_buckets)\n",
    "while events_buckets_copy:\n",
    "    random_bucket_index = random.randint(0, len(events_buckets_copy) - 1)\n",
    "\n",
    "    if not events_buckets_copy[random_bucket_index]:\n",
    "        events_buckets_copy.pop(random_bucket_index)\n",
    "        continue\n",
    "\n",
    "    all_events.append(events_buckets_copy[random_bucket_index].pop(0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = pd.DataFrame(all_events, columns=[\"process_id\", \"event_type\", \"product_id\", \"sensor_type_id\", \"source_location_id\", \"destination_location_id\", \"current_location_id\"])\n",
    "events = events.reset_index().rename(columns={\"index\": \"event_id\"})\n",
    "codes, uniques = pd.factorize(events[\"process_id\"])\n",
    "events[\"process_id\"] = codes + 1\n",
    "events.to_csv(\"output/events.csv\", header=True, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
